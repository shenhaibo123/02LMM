# 附录：术语表与模型剖析

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## A. 常用术语速查

- Token / Tokenization：文本被切分后的基本单元；常见为子词或字节。
- PPL（Perplexity）：困惑度，`exp(loss)`，越低越好。
- KV Cache：生成阶段缓存历史 Key/Value，用于加速注意力计算。
- RoPE：旋转位置编码，一种相对位置编码方案，利于长上下文外推。
- LoRA / QLoRA：低秩适配器；在量化权重上进行参数高效微调。
- RAG：检索增强生成；用外部知识检索结果增强 Prompt。
- PPO / DPO / RLAIF：偏好优化家族；强化学习或直接偏好优化。

## B. 常见陷阱与排错提示

- 数据问题：脏数据、重复样本、标签错位是训练震荡的常见原因；
- 配置错配：`vocab_size`、特殊 token id 与模型配置不一致；
- 学习率过大：loss 爆炸或出现 NaN/Inf；尝试降低 LR 与启用裁剪；
- 评测不一致：不同温度/解码策略导致差异显著；请固定评测参数。

## C. 开源模型快速剖析（示例）

- LLaMA 家族：Decoder-only + RoPE + SwiGLU；多头注意力 + RMSNorm；
- Qwen 家族：Decoder-only，GQA/MQA 以优化推理显存；中文能力强；
- DeepSeek 家族：在采样与对齐策略上有较多工程优化。

阅读技巧：

- 先看 `modeling_*.py` 的 `forward`，确定数据流；
- 再看注意力实现与位置编码；最后看训练脚本如何对接。

