# 07 推理、部署与评估

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标
 
- 理解离线推理与在线服务的差异与共性。
- 掌握常见解码策略与关键超参（greedy、temperature、top-k/top-p）。
- 了解推理吞吐优化（KV Cache、PagedAttention）与部署路径（vLLM / TGI / llama.cpp）。
- 建立基础评估体系：通用能力与专用能力如何测。
 
## 2. 推理基础概念
 
- 解码策略
  - Greedy：每步取最大概率，确定性强、易重复。
  - Temperature：调节分布平滑度，越大越“发散”，越小越“保守”。
  - Top-k / Top-p（Nucleus）：限制抽样候选集合，在多样性与可信度间权衡。
- 关键超参
  - `max_new_tokens`、`repetition_penalty`、`presence/frequency_penalty`；
  - 长文本建议结合 `stop` 规则与多段生成，避免跑偏。
 
## 3. 吞吐与延迟优化
 
- KV Cache：缓存历史 K/V，避免重复计算，生成阶段加速关键；
- PagedAttention：用分页管理显存，减少碎片并支持前缀共享；
- GQA/MQA：在不显著损失质量的前提下降低 KV Cache 开销；
- FlashAttention：v2/v3 在新架构（如 H100）上通过异步/低精度流水进一步提速；
- 推测式解码（Speculative Decoding）：小草稿模型起草、多 token 一步校验，在中低 QPS 下显著降延迟（vLLM 支持 EAGLE/草稿/MLP/n-gram 等多种路线）；
- 更进一步：编译器/张量并行/流水线并行/连续批（Continuous Batching）、Chunked Prefill、Prefix Caching。
 
## 4. 部署形态举例
 
- 本地脚本推理：直接加载权重进行生成；
- HTTP / OpenAI-API 协议服务：后端统一接口，便于前端/工具接入；
- 结合 RAG 的应用：向量检索 + Prompt 拼接 + 模型生成。
 
## 5. MiniMind 项目中的实践路径
 
- `scripts/serve_openai_api.py` 与 `scripts/chat_openai_api.py` 的角色；
- `scripts/web_demo.py`：基于 streamlit 的简易 Web 聊天界面；
- 与第三方推理引擎联动：vLLM / llama.cpp / ollama。
 
## 6. 基础评估（Benchmark）
 
- 通用能力
  - MMLU：多学科问答；看广泛知识与推理；
  - CEVAL/GAOKAO-Bench：中文考试类题库。
- 代码能力
  - HumanEval / MBPP：函数生成与单元测试通过率；
  - 注意评测时的解码策略与温度保持一致。
- 推理能力
  - GSM8K、SVAMP：数学推理；对链式思维（CoT）敏感；
  - ARC：常识与推理选择题。
- 定制评测
  - 任务/领域数据集 + 结构化打分脚本；
  - 结合自动化与少量人工抽检，形成快速回归体系。
- 评测工具
  - lm-evaluation-harness：经典通用评测框架，支持 vLLM/本地/API 多后端；
  - lighteval：覆盖 1000+ 任务、便于多后端对接与结果管理，适合快速回归。

## 7. 量化与压缩（实践向）

- 权重量化：AWQ（激活感知）、GPTQ（层级近似）、GGUF（生态格式，便于 CPU/Apple/llama.cpp）；
- 经验：追求通用质量优先 AWQ，CUDA 环境追求吞吐可选 GPTQ；本地/跨平台便捷性优先 GGUF。
 
## 8. 线上观测与安全
 
- 指标：延迟、吞吐、95/99 分位、OOM/失败率；
- 日志：请求/响应、异常栈、超时与重试链路；
- 安全：敏感实体屏蔽、拒答模板、越权检测与审计。
 
## 9. 延伸阅读与思考
 
- 推荐阅读：vLLM/TGI 官方文档与最佳实践；各大基准的评测报告；
- 思考题：如果显存很紧张，如何在吞吐、质量与多样性之间做平衡？
