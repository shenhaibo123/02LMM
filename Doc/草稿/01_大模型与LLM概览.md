# 01 大模型与 LLM 概览

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

这是一篇从「为什么大模型这么火」开始，逐步落到「语言模型到底在干什么」的入门文章。更准确一点，它是我在折腾 MiniMind 这套代码时，给自己写的总览笔记：先把大图景搞清楚，再去对应到具体的 `model/`、`trainer/` 与 `scripts/` 目录。

## 1. 本篇目标

- 搞清楚「语言模型」和「大语言模型（LLM）」分别指什么。
- 用直观类比理解：为什么只靠「预测下一个 token」就能涌现出各种“智能”行为。
- 从时间线的角度看看：从传统 NLP 到如今的预训练大模型，中间发生了哪些关键变化。
- 用一条流程线，串起 MiniMind 代码里各个脚本的大致位置。

## 2. 什么是语言模型？

如果把自然语言看成一串 token（可以是字、词或子词），那么**语言模型（Language Model）**要解决的问题非常朴素：

> 给定前面的一串 token，预测下一个最可能出现的 token 是什么。

形式上，经常会写成：

> P(next_token | previous_tokens)

在工程上，我们不会真的一次只预测一个 token，而是：

1. 把一整段文本切成 token 序列（比如长度为 512）。
2. 让模型在每个位置都预测“下一个 token”。
3. 用交叉熵等损失函数，把所有预测误差加总起来训练。

从这个角度看，语言模型更像一个超强的「自动补全」模块：

- 你打出一句话的前半句，它帮你猜后半句；
- 你写出一段代码的开头，它帮你续写剩余部分。

几乎所有的 LLM，本质上都是在做这件事，只是模型变得足够大、数据足够多，外加各种训练技巧，才有了今天这种“会聊天、会写代码、会推理”的效果。

## 3. 什么是“大语言模型（LLM）”

既然语言模型的定义如此简单，那「大」体现在哪里？

可以粗暴地看三件事：

- **参数量**：从早期几百万 / 几千万参数，到现在几十亿、上百亿甚至上千亿参数。
- **数据量**：从单一任务的小数据集，到覆盖多语言、多领域的海量文本。
- **能力边界**：从只会做「一件事」（比如情感分类），到可以**一模型多任务**，甚至能够“零样本”处理没见过的任务。

典型代表：

- GPT 系列（OpenAI）
- LLaMA / Llama 3 系列（Meta）
- Qwen 系列（阿里）
- DeepSeek 系列

这些模型往往有一整条家族谱系：不同参数规模（7B/13B/70B…），不同用途（base / instruct / chat / vlm），但底层都是一类 Transformer 解码器结构。

在 MiniMind 里，我们做的是一个「极小版本」的大语言模型：

- 参数量只有几十 M，远远小于主流大模型，但是结构上高度类似；
- 好处是：便宜、可控，可以在个人显卡上完整走一遍训练—微调—推理的流程。

## 4. 从传统 NLP 到预训练 LLM：一次范式切换

回顾一下 NLP 的发展脉络，大致可以分成几代：

### 4.1 统计语言模型：n-gram 时代

- 假设当前词只和前面 n−1 个词有关，比如：
  - 三元组（trigram）：P(w_t | w_{t-1}, w_{t-2})
- 通过统计大规模语料中 n-gram 的共现频率，估计概率。
- 问题：
  - 维度灾难：n 一大，组合数爆炸。
  - 稀疏：很多组合从未在训练语料中出现。
  - 只能建“词表级”的统计关系，无法表达更复杂的语义。

### 4.2 神经网络语言模型与词向量

后来引入了神经网络：

- 通过嵌入层（embedding）把词映射到稠密向量（word2vec、GloVe）。
- 使用前馈网络或 RNN 建模序列关系。

这一代的典型成果：

- 词向量可以捕捉一些有趣的语义关系（king - man + woman ≈ queen）。
- 但模型通常还是针对**某个具体任务**训练，迁移能力有限。

### 4.3 预训练 + 微调：一个模型统领多任务

真正的范式切换发生在**预训练（Pretrain） + 微调（Fine-tune）**出现之后：

1. 先在海量无标注文本上，做「自监督学习」（预测被 mask 掉的词 / 下一个词）。
2. 再在具体任务上用少量标注数据做微调。

好处是：

- 预训练阶段让模型学到通用语言知识；
- 微调阶段只需要为特定任务“加一点偏好”。

MiniMind 中，你可以在 `trainer/train_pretrain.py` 里看到预训练脚本的实现，而 `trainer/train_full_sft.py`、`trainer/train_lora.py` 则对应后续的指令微调阶段。

## 5. LLM 的典型应用场景

当模型规模足够大、数据足够广、训练足够稳之后，一个通用 LLM 在应用上大致可以做几类事情：

### 5.1 对话助手与代码助手

- ChatGPT、Claude 这类聊天机器人；
- GitHub Copilot、Cursor 这类代码补全与重构助手。

本质上都是「给一个上下文，让模型继续生成后续 token」，只是界面与增强机制不同。

### 5.2 各种文本生成任务

- 写摘要、改写、风格迁移；
- 文案生成、故事续写；
- 把非结构化文本整理成结构化信息。

### 5.3 信息检索与问答（RAG）

- 检索增强生成（RAG）：先从知识库中取出相关文档，再让 LLM 根据这些文档回答问题；
- 这样可以弥补模型参数中「记忆」的局限，同时便于更新知识。

在工程上，这类系统通常会：

- 使用向量数据库（如 faiss、milvus、pgvector 等）做相似度检索；
- 在调用 LLM 之前，把检索结果拼接到 prompt 中。

### 5.4 Agent 与工具调用

- 让模型根据目标自主规划调用「工具」（例如搜索、计算器、数据库查询等）；
- 模型不再只是输出一段文本，而是通过外部工具改变世界状态。

MiniMind 的体量更适合作为「算法教学与实践」载体，但思路上和这些应用是一致的。

## 6. 一个 LLM 的完整生命周期

从工程视角看，一个 LLM 的生命周期可以简单画成四个阶段：

1. **数据准备**
   - 收集、清洗、去重、切分预训练语料；
   - 构造指令微调数据、偏好数据等。
2. **预训练**
   - 在大规模语料上做自监督训练；
   - 这一阶段通常最耗算力。
3. **对齐与微调**
   - 指令微调（SFT）：让模型更「听指令」；
   - RLHF, DPO/GRPO 及其变体等：让模型更「符合人类偏好」。
4. **部署与监控**
   - 推理服务、延迟与吞吐量优化；
   - 在线监控与安全控制。

在本仓库中，大致可以这样对应：

- `dataset/`：与数据准备相关的代码。
- `model/`：模型结构与分词器定义。
- `trainer/`：预训练、SFT、LoRA、DPO、PPO/GRPO/SPO 等训练脚本。
- `scripts/`：推理与服务相关脚本（如 `serve_openai_api.py`、`chat_openai_api.py`）。

你可以把这一篇当成「地图」：先在脑中形成一个 LLM 的整体形状，然后再去读其它几篇基础知识（Tokenizer、Transformer、预训练、对齐、部署），以及对应的源码与脚本。

## 7. 大模型基础知识体系（框架）与目录

本知识体系聚焦于从零开始构建一个（多模态）大模型。我们以实践为导向，尽量精简数学理论，将重点放在核心概念、工程实践与关键技术上。以下为文档目录：

- 01）总览与核心概念（本篇）
  - [01 大模型与 LLM 概览](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/01_大模型与LLM概览.md)
- 02）基石：分词与数据工程
  - [02 分词与数据工程](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/02_分词与数据工程.md)
- 03）模型核心：Attention 与 Transformer
  - [03 模型核心：Attention 与 Transformer](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/03_模型核心_Attention与Transformer.md)
- 04）模型训练：从零到一
  - [04 模型训练：从零到一](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/04_模型训练从零到一.md)
- 05）对齐与微调：让模型更“听话”
  - [05 对齐与微调：RLHF / DPO / RLAIF](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/05_对齐与RLHF_DPO_RLAIF概览.md)
- 06）走向多模态：Omni-Model 详解
  - [06 走向多模态：Omni-Model 详解](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/06_走向多模态_Omni_Model.md)
- 07）推理、部署与评估
  - [07 推理、部署与评估](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/07_推理部署与评估.md)
- 附录
  - [附录：术语表与模型剖析](file:///Users/shenhaibo/Desktop/02LLM/Doc/基础知识/附录_术语表与模型剖析.md)

  - **Attention 机制入门**：从点积到注意力分数
    - 这一部分旨在直观理解：模型如何通过计算 Query 和 Key 的相似度（点积），并用 Softmax 归一化得到注意力权重，最后再作用于 Value，从而实现对输入序列的动态、加权聚焦。

  - **标准 Self-Attention**：Transformer 的基石
    - 原始的 Self-Attention 允许序列中的每一个 Token 都与其他所有 Token 计算注意力分数。这赋予了模型强大的全局信息捕捉能力，但也带来了 **O(n²)** 的计算和内存复杂度（n 为序列长度）。当序列变长时，这种平方级的增长会迅速成为计算瓶颈。

  - **主流 Attention 变体：为效率与长上下文而生**
    标准自注意力的二次方复杂度在处理长序列时是巨大的瓶颈。因此，社区发展出多种变体来优化效率。

    - **分组查询注意力 (Grouped-Query Attention, GQA)**
      - **核心思想**：作为对标准多头注意力 (MHA) 和多查询注意力 (MQA) 的一种折衷，GQA 旨在大幅优化推理时的显存占用。它将查询头分组，**让组内的多个查询头共享同一组键/值头**，从而显著减少需要缓存的 KV Cache 总量。
      - **代表模型**：**Qwen2** 等模型广泛采用 GQA，在保证模型质量的同时，有效提升了长上下文推理的吞吐量。

    - **稀疏注意力 与 混合专家 (MoE)**
      - **核心思想**：稀疏注意力的核心思路是让每个 Token 只关注一部分最重要的 Token，而不是全部。这一思想在 **混合专家 (MoE)** 架构中得到了大规模应用。MoE 模型包含多个“专家”网络，对于每个输入，一个门控网络会选择性地、稀疏地激活一小部分专家来处理，从而用更少的计算量实现更大的模型容量。
      - **代表模型**：**通义千问3 (Qwen3)** 系列（于2025年4月开源）是这一路线的杰出代表。其旗舰模型 `Qwen3-235B-A22B` 拥有2350亿总参数，但单次推理仅激活约220亿参数。这种**动态的稀疏激活机制**正是稀疏思想的体现，实现了性能与效率的兼得。部分Qwen3模型甚至采用了**稀疏MoE与混合注意力**的更复杂架构。

    - **线性注意力 (Linear Attention)**
      - **核心思想**：通过数学变换（如核函数）来近似原始的点积注意力，将其计算复杂度从 O(N²) 降低到 O(N)。这使得模型能处理极长的序列。
      - **代表模型**：面壁智能的 **SALA** 架构是典型应用，它巧妙地将一部分标准注意力层替换为高效的线性注意力，以应对百万级别的超长上下文挑战。

  - **Decoder-only 架构：LLaMA/Qwen/DeepSeek 等模型的设计共性**
    原始的 Transformer 模型包含一个编码器（Encoder）和一个解码器（Decoder），主要用于机器翻译等“输入序列到输出序列”的任务。然而，当前绝大多数生成式大模型（如 GPT 系列、LLaMA、Qwen 等）都采用了**纯解码器（Decoder-only）**架构。

    - **为什么是 Decoder-only？**
      - **任务目标统一**：这类模型的核心任务是**生成**，即“根据已有前文，预测下一个 Token”。这与 Decoder 的工作模式天然契合。Decoder 内部的**掩码自注意力（Masked Self-Attention）**机制，确保了在预测第 `i` 个 Token 时，只能看到 `1` 到 `i-1` 的信息，完美符合自回归（Auto-regressive）生成的设定。
      - **训练和推理简洁**：整个模型只有一个模块，结构更简单，训练和部署都更为方便。输入（Prompt）和输出（Generation）都在同一个空间中处理，可以无缝地进行“续写”。
    - **与 Encoder-Decoder 的区别**：
      - **Encoder-Decoder**：Encoder 负责双向编码整个输入序列，形成一个富含上下文的表示；Decoder 则在这个表示的引导下，单向地生成输出序列。它更适合需要完整理解输入后再进行转换的任务（如翻译、摘要）。
      - **Decoder-only**：整个模型都是单向的。无论是处理用户输入（Prompt）还是生成后续文本，都遵循着从左到右的顺序，不断地将生成的新 Token 添加到输入序列中，作为下一步预测的依据。

- **4）模型训练：从零到一**
  - **训练目标与损失函数**：模型如何学习（以 CLM 为例）
  - **关键观测指标**
    - 损失（Loss）：衡量预测与现实的差距
    - 困惑度（Perplexity, PPL）：评估语言模型性能的核心指标
  - **训练流程与优化**：梯度累积、混合精度、学习率调度

- **5）对齐与微调：让模型更“听话”**

  预训练让模型获得了渊博的知识，但并不能保证它会以我们期望的方式（如“有问必答”、“安全无害”）行事。对齐与微调，就是将模型的“能力”引导到“意图”上的过程。

  - **指令微调（Supervised Fine-Tuning, SFT）**
    - **目标**：教模型理解并遵循各种自然语言指令。
    - **做法**：收集或构造大量的“指令-回答”（Instruction-Response）数据对，然后在预训练好的模型上进行标准的监督学习微调。模型通过学习这些高质量的范例，掌握了将不同指令映射到期望输出的能力。
    - **局限**：SFT 只是模仿，它让模型“知道”该做什么，但无法系统性地让模型“偏好”某种类型的回答（例如，更喜欢“乐于助人”而不是“拒绝回答”）。此外，对整个模型进行微调（Full Fine-tuning）的计算和存储成本非常高。

  - **参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**
    为了解决全量微调的成本问题，PEFT 技术应运而生。其核心思想是在微调时冻结大部分预训练模型参数，只更新一小部分新增的、可训练的参数。

    - **LoRA (Low-Rank Adaptation)**
      - **核心思想**：大语言模型中的权重矩阵虽然维度很高，但其在微调过程中的“变化量”实际上是“低秩”的。也就是说，权重的更新可以被分解为两个更小的矩阵的乘积。
      - **做法**：对于一个预训练的权重矩阵 `W`，LoRA 冻结 `W`，并在旁边新增两个可训练的、低秩的“适配器”矩阵 `A` 和 `B`。模型的前向传播变为 `W*x + B*A*x`。微调时，只更新 `A` 和 `B` 的参数，它们的参数量远小于 `W`。
      - **优势**：极大减少了需要训练的参数量（通常不到原模型的 1%），显著降低了显存占用。微调完成后，只需保存小小的适配器权重，便于分享和部署。

    - **QLoRA (Quantization + LoRA)**
      - **核心思想**：在 LoRA 的基础上，进一步通过“量化”技术来压缩被冻结的基础模型，从而节省更多显存。
      - **做法**：
        1. **量化**：将 16 位或 32 位的预训练模型权重，量化到更低的精度，如 4-bit。
        2. **训练**：在 4-bit 的模型上添加 LoRA 适配器进行训练。
        3. **反量化**：在计算梯度时，将 4-bit 的权重动态地反量化回高精度，以保证训练的稳定性和效果。
      - **优势**：使得在单张消费级显卡（如 24GB VRAM）上微调数十亿参数的大模型成为可能，极大地降低了微调的硬件门槛。

  - **偏好学习：从 RLHF 到 DPO/GRPO**
    SFT 和 PEFT 教会了模型“如何做”，而偏好学习则告诉模型“什么更好”。

    - **RLHF (Reinforcement Learning from Human Feedback)**
      - **目标**：让模型的输出更符合人类的综合偏好（如更有用、更无害、更诚实）。
      - **做法（三阶段）**：
        1. **SFT**：先进行指令微调，得到一个基础模型。
        2. **训练奖励模型（Reward Model, RM）**：让 SFT 模型对同一个提示（Prompt）生成多个不同回答，然后由人类对这些回答进行排序。用这些排序数据，训练一个奖励模型，让它学会给“好的”回答打高分，给“坏的”回答打低分。
        3. **强化学习**：将奖励模型作为“环境”，使用强化学习算法（如 PPO）来微调 SFT 模型。模型通过不断尝试生成回答并从奖励模型那里获得“分数”反馈，来学习如何最大化奖励，从而使其输出逐渐对齐人类偏好。
      - **挑战**：流程复杂，涉及多个模型的训练，且强化学习过程不稳定，调试困难。

    - **DPO (Direct Preference Optimization)**
      - **核心思想**：RLHF 的目标本质上是让“赢家”回答的概率高于“输家”。DPO 发现，这个过程可以被巧妙地转化为一个简单的二元分类问题，从而完全绕开复杂的强化学习循环。
      - **做法**：直接使用人类偏好数据（即“提示-赢家回答-输家回答”三元组），通过一个特殊的损失函数，直接优化语言模型本身。这个损失函数的目标是：**最大化模型对“赢家”回答的似然度，同时最小化对“输家”回答的似然度**。
      - **优势**：将对齐过程简化为了类似 SFT 的监督学习，训练过程更稳定、更高效，不再需要独立的奖励模型，也不再需要复杂的强化学习步骤。

- **6）走向多模态：Omni-Model 详解**

  纯文本的 LLM 只能理解世界的一个侧面，而真正的智能需要整合来自多种“感官”的信息。多模态（Multimodality）正是为了解决这个问题，目标是让模型能够同时理解和处理文本、图像、音频、视频等多种类型的数据。

  - **什么是多模态？**
    多模态学习旨在构建能够从多种模态中学习和推理的模型。对于大模型而言，这意味着不仅仅把世界看作是文本序列，而是将其视为一个由文本、像素、声音等共同构成的多维信息空间。

  - **核心思路：如何对齐不同模态的信息？**
    不同模态的数据“语言”是不同的（例如，图像是像素矩阵，文本是 Token 序列）。多模态大模型的关键挑战，在于如何将这些异构的数据，映射到一个统一的、可供 LLM 理解的表示空间。

    当前主流的方案，特别是对于**视觉-语言模态（Vision-Language）**，通常遵循以下范式：
    1. **使用一个独立的视觉编码器（Vision Encoder）**：通常会选用一个强大的、预训练好的视觉模型（如 ViT，源于 CLIP 或其他模型）来提取图像的深层特征。这个编码器负责将输入的图片“看懂”，并转换成一组特征向量。
    2. **设计一个投影层（Projection Layer）**：这个模块（通常是一个小型的 MLP 网络）作为“翻译官”，负责将视觉编码器输出的特征向量，线性或非线性地投影（映射）到与语言模型相同的向量空间中。
    3. **将视觉特征“注入”语言模型**：转换后的视觉特征向量，就像普通的文本 Token Embedding 一样，被送入 LLM 的 Transformer 结构中。这样，LLM 在处理文本的同时，也能“看到”图像的内容，从而实现对图文的联合理解和推理。

  - **代表方案概览：从 LLaVA 到 Qwen-VL**

    - **LLaVA (Large Language and Vision Assistant)**：作为早期开源多模态模型的典范，LLaVA 的结构清晰地体现了上述核心思路。
      - **架构**：它使用一个预训练的 CLIP ViT 作为视觉编码器，一个简单的线性投影层作为“翻译官”，并将其与 Vicuna（一个指令微调的 LLaMA 模型）连接起来。
      - **训练**：LLaVA 的一个重要贡献是提出了“视觉指令微调（Visual Instruction Tuning）”的概念。它通过两阶段训练来高效地对齐模态：
        1. **第一阶段（特征对齐）**：冻结视觉编码器和 LLM 的大部分参数，只训练投影层。目标是让投影层学会如何将图像特征快速、准确地翻译成 LLM 能理解的“语言”。
        2. **第二阶段（指令微调）**：将投影层与 LLM 的参数一起进行端到端的微调。使用包含图像的、更复杂的指令跟随数据，教模型如何根据图文上下文进行对话和推理。
      - **启示**：LLaVA 证明了，通过一个轻量级的投影层和精心设计的指令微调数据，可以低成本、高效率地“解锁”一个已有的强大 LLM 的多模态能力。

    - **Qwen-VL (通义千问-VL)**：作为更进一步的工业级方案，Qwen-VL 在 LLaVA 的基础上做了诸多改进，展现了更强的性能和更广泛的应用场景。
      - **架构**：它同样采用了视觉编码器 + 投影层 + LLM 的设计，但其视觉编码器和 LLM 都经过了更深入的定制和优化。
      - **关键特性**：
        - **动态分辨率**：与 LLaVA 通常使用固定分辨率的图像不同，Qwen-VL 支持处理任意分辨率的图像，并将其映射为动态数量的视觉 Token。这使得模型能更灵活地处理各种尺寸和长宽比的图片，更接近人类视觉。
        - **更强的基础模型**：Qwen-VL 基于 Qwen（通义千问）系列模型构建，其本身就具备强大的多语言和代码能力，为多模态的深入理解提供了坚实的基础。
        - **多语言支持**：能够理解和处理图像中嵌入的多种语言文字，实现了真正的多语言多模态。

  - **Omni-Model：迈向通用智能的未来**
    LLaVA 和 Qwen-VL 主要聚焦于图文两种模态。而 **Omni-Model**（全能模型）则代表了更大胆的愿景：构建一个能够统一处理和理解**所有**主流模态（文本、图像、视频、音频等）的单一模型。

    - **核心理念**：通过统一的 Tokenizer 和 Transformer 架构，将所有模态的数据都编码为统一的序列化表示，实现“一模通万物”。
    - **实现路径**：这通常需要更复杂的模型结构（如为不同模态设计专门的编码器或 Adapter），以及更庞大、更多样的多模态联合训练数据。例如，通过“渐进式模态对齐”策略，先对齐图文，然后引入视频（作为图像序列），再引入音频，逐步扩大模型能理解的模态范围。
    - **意义**：Omni-Model 被视为通向更通用人工智能（AGI）的关键一步。当模型能够像人一样同时接收和处理来自视觉、听觉、语言的综合信息时，它才能真正开始理解我们这个复杂而丰富的物理世界。

- **7）推理、部署与评估**

  模型训练完成后，如何高效地让其提供服务，并科学地评估其表现，是决定其价值的最后一环。

  - **解码策略**：Greedy, Temperature, Top-k/Top-p
    - **Greedy Search（贪心搜索）**：最简单的策略。在每一步都选择概率最高的那个 Token。缺点是容易生成重复、缺乏变化的“安全”文本，可能会陷入局部最优。
    - **Sampling with Temperature**：为了增加多样性，我们引入“温度”（Temperature）参数。在 Softmax 之前，将 Logits 除以一个温度值。T > 1 会使概率分布更平滑，鼓励模型探索更多可能性；T < 1 则会使分布更尖锐，接近贪心搜索。
    - **Top-k & Top-p (Nucleus) Sampling**：为了在多样性和合理性之间取得更好的平衡，这两种策略被提出。
      - **Top-k**：在每一步，只从概率最高的 k 个 Token 中进行抽样。
      - **Top-p**：只从累积概率加起来刚好超过 p 的最小 Token 集合中进行抽样。这是一种动态的 Top-k，k 的大小会根据上下文的确定性而变化。

  - **推理加速：从 KV Cache 到 PagedAttention**
    LLM 的自回归生成过程天生是串行的，每生成一个新 Token，都需要基于之前的所有 Token 进行一次完整的模型前向计算。这个过程非常慢，推理优化至关重要。

    - **KV Cache**
      - **问题**：在生成第 `i+1` 个 Token 时，标准的 Attention 机制需要利用前面 `i` 个 Token 的 Key 和 Value。如果从头计算，会有大量的重复计算。
      - **解决方案**：引入 **Key-Value Cache**。在计算完第 `i` 个 Token 的 Attention 后，将其对应的 Key 和 Value 向量缓存到 GPU 显存中。在下一步计算时，新的 Token 只需要计算自己的 Query，然后与缓存中所有历史的 K/V 进行 Attention 计算即可，无需再为历史 Token 重复计算 K/V。
      - **效果**：极大地加速了生成过程。这是所有现代 LLM 推理框架的基础优化。
      - **新的问题**：KV Cache 虽然解决了计算问题，但引入了严重的**显存管理问题**。由于无法预知生成文本的长度，系统通常需要预先分配一块巨大的、连续的显存空间来存放 KV Cache，导致大量浪费（内部碎片和外部碎片）。研究表明，这种朴素的内存管理方式，可能导致 60%-80% 的显存被浪费。

    - **PagedAttention**
      - **核心思想**：借鉴操作系统中“虚拟内存”和“分页”的思想来管理 KV Cache。
      - **做法**：不再为每个请求分配一个巨大的连续显存块，而是将 KV Cache 存储在非连续的、固定大小的“物理块”（Block）中。同时，维护一个“逻辑块表”，将序列中的 Token 位置映射到对应的物理块。
      - **优势**：
        - **解决内存碎片**：由于块的大小是固定的，可以像管理内存页一样高效管理显存，几乎完全消除了内存浪费。
        - **高效的内存共享**：对于复杂解码策略（如 Beam Search）或并行请求，如果多个序列共享相同的前缀（Prompt），它们的 KV Cache 可以直接指向相同的物理块，实现了“写时复制”（Copy-on-Write），极大节省了显存。
      - **代表实现**：`vLLM` 项目率先提出了 PagedAttention，并凭此实现了业界领先的推理吞吐量。

  - **基础评估**：如何在标准集上评估模型性能
    - 介绍常用的评估基准（Benchmark）和指标，如 MMLU（综合能力）、HumanEval（代码能力）、ARC（推理能力）等，以及它们各自的侧重点。


- **附录**
  - 术语表与常见陷阱
  - 典型开源模型剖析：LLaMA、Qwen、DeepSeek

以上框架为后续各篇提供「地图坐标」：遇到新概念，先定位其所属层，再回到对应文档与源码钻研，可显著降低知识碎片化。

**参考框架与资料**：
- Datawhale《Happy-LLM》项目（系统性 LLM 原理与实践）
- Datawhale 大模型课程食用指南（学习路径梳理）
- 《2025 年最新大模型学习路线》（公开课与实践清单）

## 8. 延伸阅读与思考

- **推荐阅读**：
  - 主流开源模型（Llama 3、Qwen、DeepSeek 等）的技术报告与 README。
  - 本仓库上游项目 MiniMind 的原始说明文档，用来对比两边的设计取舍。
- **思考题**：
  - 仅仅通过「预测下一个 token」这一个目标，为什么会涌现出对话、推理、写代码等复杂能力？
  - 如果把模型规模缩小很多（比如像 MiniMind 一样几十 M），哪些能力会最先消失？哪些能保留？
