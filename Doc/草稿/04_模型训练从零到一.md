# 04 模型训练：从零到一

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标

- 明确自回归语言建模（CLM）的训练目标与损失。
- 掌握关键观测指标：Loss、Perplexity（PPL）、学习率与收敛信号。
- 理解影响训练稳定性与效率的核心技术：梯度累积、混合精度、学习率调度、Packing。
- 结合本仓库脚本，打通从数据到训练再到简单评估的闭环。

## 2. 训练目标与损失函数

- 目标：给定输入序列 `x[0..T-1]`，最大化每个位置预测下一个 token 的似然。
- 工程实现：一次前向让每个位置都预测“下一个 token”，用交叉熵把所有位置的误差加总。
- 损失与 PPL：
  - `Loss` 是平均负对数似然；
  - `PPL = exp(Loss)`，可直观理解为“平均有多少个等可能选择”的困惑度。

## 3. 数据与打包（Packing）

- 数据清洗与去重：减少模板化与重复文本，保证“有效多样性”。
- Packing：将多条样本拼接成长序列，再按固定长度切块，显著减少 padding 浪费。
- Label shift：对每个切块，`labels` 通常是 `input_ids` 右移一位，并在起始位置打上 `-100` 屏蔽 loss。

## 4. 训练流程与关键技巧

- 批量与累积
  - 物理 `batch_size` × `gradient_accumulation_steps` ≈ 有效全局 batch；
  - 累积可以在显存有限时模拟更大的 batch，稳定梯度估计。
- 混合精度（AMP / bfloat16）
  - 大幅提升吞吐与显存利用；
  - 注意 loss scale、溢出与 NaN 监控。
- 学习率与调度器
  - 常见：warmup + cosine 或 linear decay；
  - 经验：warmup 占总步数 1%–3%，初始 LR 与 batch/模型大小联动。
- 正则化与稳定性
  - Weight Decay（L2）、Dropout；
  - 梯度裁剪（如 1.0）防止发散；
  - EMA/SGD seldom used in LLM pretrain，相比 AdamW 更常用。

### 4.1 分布式与显存优化

- 分布式并行：数据并行（DP）、张量并行（TP）、流水线并行（PP）、专家并行（EP，MoE）；
- ZeRO 优化：参数/梯度/优化器状态切分，减少单卡显存（DeepSpeed）；
- Gradient Checkpointing：以计算换显存；结合 FlashAttention 可显著降低峰值；
- Fused/8-bit 优化器：如 fused AdamW、bitsandbytes，减少 CPU/GPU 带宽瓶颈；
- PyTorch 2.x 编译与调度：`torch.compile`、异步 dataloader、pin memory 提升吞吐。

### 4.2 参数高效微调（PEFT）

- LoRA：在大权重旁插入低秩适配器，训练/保存成本低；
- QLoRA：在 4-bit 冻结权重上训练 LoRA，进一步降低显存；
- 选择策略：小数据/快速迭代优先 LoRA；需求稳定后再考虑全量合并。

## 5. 微调基础：SFT（简述）

- 数据形态：`指令 → 回复`，可拼接系统提示与多轮对话；
- 常见模板：将多字段整理为单序列，并在不同段落之间插入特殊标记；
- 训练要点：保持 `bos/eos`、`pad` 与损失屏蔽规则一致；控制输出段落参与 loss 的范围。

更系统的对齐与偏好优化详见 05 篇。

## 6. 训练曲线的阅读

- Loss 连续下降、PPL 持续降低是基本信号；
- 若训练/验证两侧差距扩大，可能过拟合（尤其在小数据 SFT）；
- 若长期震荡无下降，检查学习率、数据质量与 batch 设置。

## 7. 与本仓库脚本的对应

- 预训练：`trainer/train_pretrain.py`
  - 负责加载打包后的预训练数据；
  - 构造自回归标签与 loss；
  - 支持梯度累积、AMP 与调度器。
- 全量 SFT：`trainer/train_full_sft.py`
  - 适合小模型或资源充足场景；
  - 直接更新全部参数，迭代收敛快但显存与存储成本高。
- LoRA 微调：`trainer/train_lora.py`
  - 仅训练低秩适配器，显著降低显存与保存开销；
  - 适合多数实际任务，结合 05 篇了解对齐策略更佳。

## 8. 基本排错清单

- 数据维度不匹配：检查 `max_length`、`attention_mask` 与 `labels` 的齐次；
- `nan`/`inf`：降低学习率，开启梯度裁剪，检查 AMP 配置；
- 显存爆：减小 batch 或 seq_len；启用 gradient checkpointing。

## 9. 延伸阅读与思考

- 推荐阅读：Chinchilla-style Scaling（数据/参数/算力配比）、FlashAttention 系列、训练基础设施（DeepSpeed/Accelerate）。
- 思考题：
  - 相同算力预算下，应该优先扩大数据、延长训练，还是增大模型？
  - 对于几十 M 的小模型，`seq_len` 和 `vocab_size` 的取舍该怎么做？
