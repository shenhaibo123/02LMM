# 05 对齐与微调：RLHF / DPO / RLAIF

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标

- 理解「对齐（Alignment）」在大模型中的含义与目标。
- 从整体上把握 RLHF、DPO、RLAIF 等方法的流程与差异。
- 了解 MiniMind 项目中相关训练脚本的基本角色与数据格式。

## 2. 为什么需要对齐

- 仅靠预训练模型存在的问题：有害、跑偏、不守指令。
- 「有能力但不听话」与「有能力又听话」的区别。
- 对齐的三大目标：Helpful / Honest / Harmless（有用、诚实、无害）。
- SFT 的作用与局限：教模型“做什么”，但难以精细塑造“更偏好的答案风格与边界”。

## 3. 方法全景

### 3.1 RLHF（三阶段）

- 阶段 A：得到初始策略模型（通常是预训练 + SFT）。
- 阶段 B：构建偏好数据并训练奖励模型（RM）：
  - 数据形态：同一条提示 `prompt` 对应多个回答，人工对其排序或选优/选劣；
  - 训练：让 RM 学会给「更好」的回复更高分。
- 阶段 C：基于 RM 用强化学习（如 PPO）优化策略：
  - 策略模型生成 → RM 打分 → 算优势/梯度 → 更新策略；
  - 优点：可表达复杂偏好；难点：训练不稳定与调参复杂。

### 3.2 DPO（Direct Preference Optimization）

- 思想：直接把“赢家优于输家”的约束转为一个可微的目标函数；
- 做法：使用三元组数据（prompt, chosen, rejected），最大化 chosen 的似然、最小化 rejected 的似然；
- 优点：不需要 RM 与强化学习，流程接近监督学习，稳定高效；
- 常见超参：温度/β 控制惩罚强度，防止过度惩罚造成模式崩塌。

### 3.3 RLAIF / GRPO / SPO 等

- RLAIF：用强模型或多模型投票生成偏好，减少人工成本；
- GRPO / SPO：在 DPO 思路上调整目标，寻求更好的稳定性/样本效率；
- 工程上常与 SFT/LoRA 结合，快速在特定风格与安全边界上微调。

### 3.4 新近变体与组合范式（2024–2026）

- ORPO（Odds-Ratio Preference Optimization）：在几率空间进行缩放，缓解偏好样本不均衡带来的梯度稀释，更稳健地处理长尾/小众分布。
- SimPO（Simple Preference Optimization）：弱化对参考分布的显式依赖，梯度更稳定，适合嘈杂或众包偏好数据。
- KTO：受前景理论启发的非对称偏好优化，对高风险错误施以更大惩罚，适用于合规/医疗/金融等高风险域。
- Step-/Token-level DPO：将“赢家优于输家”的约束细化到推理步骤或关键 token，提升链式思维与数学推理。
- 组合实践范式（经验法则）：SFT → SimPO 做“广义人格” → ORPO 处理类别不均衡 → KTO 聚焦高风险域 → DPO 精修高质量黄金对。

## 4. 数据与模板

- 偏好数据基本字段：
  - `prompt` / `chosen` / `rejected`（或 `A/B + 赢家指示`）；
  - 可包含系统提示、多轮上下文与元信息（来源、标注者、质量等级）。
- 常见数据约束：
  - 严格去重与清洗，避免泄露评测集合；
  - 对有害内容与敏感场景设置分流与过滤策略。

## 5. 与本仓库脚本的对应

- DPO：`trainer/train_dpo.py`
  - 接受偏好数据三元组；
  - 支持全量参数或 LoRA 方式优化。
- RLHF 家族：
  - `trainer/train_ppo.py`（如有）：基于 PPO 的策略优化；
  - `trainer/train_grpo.py`、`trainer/train_spo.py`：偏好优化的不同目标变体；
  - 结合 `serve_openai_api.py` 可在线采样生成候选并做半自动打分。

## 6. 实践建议

- 先做一轮高质量 SFT 保底，再接 DPO/RLAIF；
- 偏好损失系数与学习率要小心调节，逐步增大，防止“遗忘”基础能力；
- 构造对抗/灰区样本，关注拒答/重定向等安全策略的具体实现；
- 小模型建议优先 LoRA，快速试错，验证有效性后再考虑全量/合并权重。

## 7. 延伸阅读与思考

- 推荐阅读：OpenAI InstructGPT / RLHF 系列博客、DPO 原始论文与后续复现；ORPO/SimPO/KTO 等近年方法小结与综述（可参考 DPO 系列综述与 RainbowPO 等工作）；
- 思考题：
  - 如果没有人工偏好数据，能否完成「对齐」？在你关心的任务上如何近似替代？
  - DPO 与 RLHF 的优缺点在你的硬件预算与数据条件下如何取舍？
