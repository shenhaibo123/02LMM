# 06 走向多模态：Omni-Model 详解

> 本文档由 Vibe Writing 大模型生成初稿，并结合本仓库实践与个人学习需求进行整理与校订。

## 1. 本篇目标

- 理解“多模态”的基本概念与工程范式。
- 掌握图文多模态的主流架构：视觉编码器 + 投影层 + LLM。
- 了解 LLaVA、Qwen-VL 等代表方案与训练阶段。
- 展望 Omni-Model：统一文本、图像、音频与视频的通用模型。

## 2. 为什么需要多模态

- 纯文本只能表达世界的一部分；图像、音频、视频提供更丰富的上下文；
- 多模态让模型在更接近现实的场景中理解与决策：读图表、看 UI、理解场景与流程。

## 3. 标准范式：视觉编码器 → 投影层 → LLM

- 视觉编码器（Vision Encoder）
  - 常用：CLIP ViT、SigLIP、DINO、SAM 家族等；
  - 产出：一组图像特征向量（patch/grid/tokens）。
- 投影层（Projector）
  - 作用：把视觉特征映射到与语言模型相同的隐空间维度；
  - 形式：线性层或小型 MLP；早期只训练这一小块以完成“特征对齐”。
- 注入 LLM
  - 把投影后的视觉 token 与文本 token 拼接输入；
  - 通过特殊分隔符/位置编码，让 LLM 同时“看见”图像与文字。

## 4. 两阶段训练：以 LLaVA 为例

- 阶段 1：对齐（Feature Alignment）
  - 冻结视觉编码器与 LLM，大多只训练投影层；
  - 用合成的“图像描述”或配对数据快速让两域对齐。
- 阶段 2：视觉指令微调（Visual Instruction Tuning）
  - 解冻部分或全部 LLM 参数，与投影层联合微调；
  - 数据来自 GUI 问答、图表理解、OCR、跨模态推理等。

## 5. 工业方案：Qwen-VL 的关键特性

- 动态分辨率：将任意分辨率映射为动态数量的视觉 token；
- 更强基础模型：基于 Qwen 系列，具备多语言与代码能力；
- OCR 友好：对图中文本有更好的识别与理解能力。

### 5.1 LLaVA 系列的最新进展

- LLaVA-OneVision：统一单图、多图与视频任务，强调跨场景迁移与“视觉 token 预算”的均衡设计，便于在不同场景间复用能力（2024）。
- LLaVA-Video：合成高质量视频指令数据进行微调，显著提升视频理解与问答能力；实践中常在 OneVision 基座上增量适配（2024）。
- 训练提示：阶段 1 对齐投影层；阶段 2 用高质量视觉指令数据做联合微调；必要时加入纯文本 SFT 维持语言能力。

## 6. Omni-Model：统一多模态的路径

- 统一 Tokenizer 与隐空间：将文本、图像/音频特征统一为序列；
- 分步对齐：先图文，再引入视频（图像序列）、音频（时序特征）；
- 关键挑战：对齐数据规模、跨模态位置编码、上下文长度与吞吐优化。

## 7. 与本仓库的关系

- 本仓库以文本 LLM 为主；多模态思路可用于扩展实验：
  - 增加视觉编码器与投影层模块；
  - 在 `trainer` 中加入图文混合数据管线；
  - 参考 LLaVA 的两阶段流程完成最小可行实现。

## 8. 延伸阅读与思考

- 推荐阅读：LLaVA、Qwen-VL、MiniCPM-V 系列论文与开源实现；
- 思考题：
  - 若只允许新增很小的参数量，如何最有效地“接入”视觉？
  - 在你的应用中，图文耦合的最小实验集是什么？如何构造？
