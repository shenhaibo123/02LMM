# 评测与部署

本文档整合自 [MiniMind](https://github.com/jingyaogong/minimind) 的 RoPE 长度外推、客观评测与多种部署方式，便于在 02LMM 中复用。

---

## 一、RoPE 长度外推（YaRN）

模型支持通过 **YaRN** 算法对 RoPE 位置编码做长度外推，使模型能处理超出训练长度的文本。

### 1. 原生 PyTorch 权重（eval_llm.py）

推理时加上 `--inference_rope_scaling` 即可启用 RoPE 外推：

```bash
python eval_llm.py --weight full_sft --inference_rope_scaling
```

### 2. Transformers 格式（config.json）

若已用 `scripts/convert_model.py` 转为 HuggingFace/Transformers 格式，可在保存的 `config.json` 中增加：

```json
"rope_scaling": {
    "type": "yarn",
    "factor": 16.0,
    "original_max_position_embeddings": 2048,
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "attention_factor": 1.0
}
```

启用后，在长文本上的困惑度（PPL）通常会有明显改善。

---

## 二、客观评测（lm-evaluation-harness）

使用 [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) 做标准选择题评测（如 C-Eval、C-MMLU、A-CLUE、TMMLU+）。选择题常用做法：取 A/B/C/D 对应 token 的预测概率，选概率最大的选项与标准答案比较正确率。

### 安装

```bash
pip install lm_eval
```

### 命令行评测（Transformers 模型）

```bash
lm_eval --model hf \
  --model_args pretrained=<模型路径>,device=cuda,dtype=auto \
  --tasks ceval* \
  --batch_size 8 \
  --trust_remote_code
```

可替换任务为 `cmmlu*`、`aclue*`、`tmmlu*` 等。小参数量模型在这些榜单上多集中在 25% 附近（接近随机），结果仅供对照。

### 本项目封装

```bash
# 本地 HF 格式模型
python scripts/eval_model_benchmark.py --backend hf --model-path <path> --tasks ceval* --device cuda:0

# 或先启动 OpenAI 兼容 API，再 --backend api 评测
python scripts/serve_openai_api.py --device cuda:0
python scripts/eval_model_benchmark.py --backend api --tasks hellaswag gsm8k
```

---

## 三、模型转换

`scripts/convert_model.py` 支持 **PyTorch 原生权重 ↔ Transformers** 互转，便于接入 HF 生态与下游推理框架。

- 将 `out/` 下 `full_sft_512.pth` 等转为 HF 格式后，可被 lm_eval、vLLM、llama.cpp 等直接加载。
- 使用方式见脚本内注释与 README「项目结构」中的 convert_model 说明。

---

## 四、OpenAI 兼容 API 服务

`scripts/serve_openai_api.py` 提供兼容 OpenAI API 的聊天接口，可对接 FastGPT、Open-WebUI、Dify 等。

### 启动

```bash
python scripts/serve_openai_api.py --device cuda:0   # 或 mps / cpu
```

### 测试

```bash
python scripts/chat_openai_api.py
```

### curl 示例

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model-identifier",
    "messages": [ { "role": "user", "content": "世界上最高的山是什么？" } ],
    "temperature": 0.7,
    "max_tokens": 512,
    "stream": true
  }'
```

---

## 五、vLLM

[vLLM](https://github.com/vllm-project/vllm) 适合高吞吐推理。需先将权重转为 Transformers 格式（见第三节）。

```bash
vllm serve ./path/to/transformers_model --model-impl transformers --served-model-name "k" --port 8998
```

---

## 六、llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp) 支持 CPU/GPU、命令行与量化。

1. 将 llama.cpp 与 02LMM 放在同级目录，按官方步骤编译。
2. 在 `convert_hf_to_gguf.py` 的 `get_vocab_base_pre` 末尾为 MiniMind tokenizer 增加分支（若为 None 可设 `res = "qwen2"` 等）。
3. 转换：`python convert_hf_to_gguf.py ../02LMM/path/to/transformers_model`
4. 可选量化：`./build/bin/llama-quantize .../model.gguf .../Q4-model.gguf Q4_K_M`
5. 推理：`./build/bin/llama-cli -m .../model.gguf -sys "You are a helpful assistant"`

---

## 七、Ollama

1. 用 llama.cpp 得到 gguf 后，在模型目录下写 `minimind.modelfile`：

```
FROM ./Q4-MiniMind2.gguf
SYSTEM """You are a helpful assistant"""
TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ .Response }}<|im_end|>
"""
```

2. 创建并运行：`ollama create -f minimind.modelfile minimind-local`，`ollama run minimind-local`。
3. 也可直接使用社区镜像：`ollama run jingyaogong/minimind2`。

---

## 八、MNN（端侧推理）

[MNN](https://github.com/alibaba/MNN) 面向端侧，支持 4bit HQQ 等量化导出。

- 进入 `MNN/transformers/llm/export`，使用官方导出脚本将 Transformers 模型导出为 MNN 格式。
- 在 Mac 或手机上用 `llm_demo` 或官方 APP 加载导出的 config 与权重进行测试。

---

以上内容整理自 MiniMind 文档与 02LMM 现有脚本，具体以各仓库最新文档为准。
